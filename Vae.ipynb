{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d1570-c29e-4c0e-b830-4b7a41bc3a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "num_samples_normal = 10000\n",
    "num_samples_anomaly = 200\n",
    "num_features = 10\n",
    "\n",
    "mean_normal = np.zeros(num_features)\n",
    "std_normal = np.ones(num_features)\n",
    "normal_data = np.random.normal(loc=mean_normal, scale=std_normal, size=(num_samples_normal, num_features))\n",
    "\n",
    "mean_anomaly = np.full(num_features, 5.0)\n",
    "std_anomaly = np.full(num_features, 1.5)\n",
    "anomalous_data = np.random.normal(loc=mean_anomaly, scale=std_anomaly, size=(num_samples_anomaly, num_features))\n",
    "\n",
    "combined_data = np.vstack((normal_data, anomalous_data))\n",
    "\n",
    "labels = np.zeros(num_samples_normal + num_samples_anomaly)\n",
    "labels[num_samples_normal:] = 1\n",
    "\n",
    "feature_names = [f'feature_{i+1}' for i in range(num_features)]\n",
    "\n",
    "df_synthetic = pd.DataFrame(combined_data, columns=feature_names)\n",
    "df_synthetic['anomaly'] = labels\n",
    "\n",
    "print(f\"Synthetic dataset created with {df_synthetic.shape[0]} samples and {df_synthetic.shape[1] - 1} features.\")\n",
    "print(f\"Number of normal samples: {np.sum(df_synthetic['anomaly'] == 0)}\")\n",
    "print(f\"Number of anomalous samples: {np.sum(df_synthetic['anomaly'] == 1)}\")\n",
    "print(\"First 5 rows of the synthetic dataset:\")\n",
    "print(df_synthetic.head())\n",
    "print(\"Last 5 rows of the synthetic dataset (expected anomalies):\")\n",
    "print(df_synthetic.tail())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df_synthetic.drop('anomaly', axis=1)\n",
    "y = df_synthetic['anomaly']\n",
    "\n",
    "print(f\"Original dataset shape: {X.shape}, Labels shape: {y.shape}\\n\")\n",
    "\n",
    "X_normal = X[y == 0]\n",
    "y_normal = y[y == 0]\n",
    "\n",
    "X_anomaly = X[y == 1]\n",
    "y_anomaly = y[y == 1]\n",
    "\n",
    "print(f\"Normal data shape: {X_normal.shape}, Anomalous data shape: {X_anomaly.shape}\\n\")\n",
    "\n",
    "X_train_normal, X_temp_normal, y_train_normal, y_temp_normal = train_test_split(\n",
    "    X_normal, y_normal, test_size=0.3, random_state=42\n",
    ")\n",
    "X_val_normal, X_test_normal_only, y_val_normal, y_test_normal_only = train_test_split(\n",
    "    X_temp_normal, y_temp_normal, test_size=0.5, random_state=42 # 0.5 of 0.3 is 0.15\n",
    ")\n",
    "\n",
    "print(f\"Normal training data shape: {X_train_normal.shape}\")\n",
    "print(f\"Normal validation data shape: {X_val_normal.shape}\")\n",
    "print(f\"Normal test (only normal) data shape: {X_test_normal_only.shape}\\n\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_normal)\n",
    "X_val_scaled = scaler.transform(X_val_normal)\n",
    "X_anomaly_scaled = scaler.transform(X_anomaly)\n",
    "X_test_normal_only_scaled = scaler.transform(X_test_normal_only)\n",
    "\n",
    "print(\"Features scaled using StandardScaler fitted on training data.\\n\")\n",
    "\n",
    "X_test_final_scaled = np.vstack((X_test_normal_only_scaled, X_anomaly_scaled))\n",
    "y_test_final = np.hstack((y_test_normal_only, y_anomaly))\n",
    "\n",
    "print(f\"Final X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"Final y_train_normal shape: {y_train_normal.shape}\")\n",
    "print(f\"Final X_val_scaled shape: {X_val_scaled.shape}\")\n",
    "print(f\"Final y_val_normal shape: {y_val_normal.shape}\")\n",
    "print(f\"Final X_test_final_scaled shape: {X_test_final_scaled.shape}\")\n",
    "print(f\"Final y_test_final shape: {y_test_final.shape}\")\n",
    "\n",
    "num_anomalies_in_test = np.sum(y_test_final == 1)\n",
    "num_total_in_test = len(y_test_final)\n",
    "print(f\"Number of anomalies in final test set: {num_anomalies_in_test} ({(num_anomalies_in_test/num_total_in_test)*100:.2f}%) \")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "print(\"TensorFlow and Keras libraries imported successfully.\")\n",
    "\n",
    "original_dim = X_train_scaled.shape[1]\n",
    "\n",
    "latent_dim = 5\n",
    "\n",
    "print(f\"Original input dimension: {original_dim}\")\n",
    "print(f\"Latent dimension: {latent_dim}\")\n",
    "\n",
    "input_tensor = keras.Input(shape=(original_dim,), name='encoder_input')\n",
    "\n",
    "x = layers.Dense(64, activation='relu')(input_tensor)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "encoder = Model(input_tensor, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "print(\"Encoder and reparameterization trick implemented successfully.\")\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,), name='decoder_input')\n",
    "\n",
    "y = layers.Dense(32, activation='relu')(latent_inputs)\n",
    "y = layers.Dense(64, activation='relu')(y)\n",
    "\n",
    "reconstruction = layers.Dense(original_dim, activation='linear', name='decoder_output')(y)\n",
    "\n",
    "decoder = Model(latent_inputs, reconstruction, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "print(\"Decoder implemented successfully.\")\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, original_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.original_dim = original_dim\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.mean_squared_error(data, reconstruction)\n",
    "            ) * self.original_dim\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "            )\n",
    "\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            keras.losses.mean_squared_error(data, reconstruction)\n",
    "        ) * self.original_dim\n",
    "\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "vae = VAE(encoder, decoder, original_dim)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.build(input_shape=(None, original_dim))\n",
    "vae.summary()\n",
    "\n",
    "print(\"Full VAE model implemented and compiled successfully.\")\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, original_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.original_dim = original_dim\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                self.mse_loss_fn(data, reconstruction)\n",
    "            ) * self.original_dim\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "            )\n",
    "\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            self.mse_loss_fn(data, reconstruction)\n",
    "        ) * self.original_dim\n",
    "\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "vae = VAE(encoder, decoder, original_dim)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.build(input_shape=(None, original_dim))\n",
    "\n",
    "print(\"VAE class definition updated and model re-instantiated with fix for MSE loss.\")\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "print(f\"Starting VAE training for {epochs} epochs with a batch size of {batch_size}...\")\n",
    "\n",
    "history = vae.fit(\n",
    "    X_train_scaled,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val_scaled,),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"VAE model training completed.\")\n",
    "\n",
    "print(\"Predicting latent space parameters and reconstructions for the test set...\")\n",
    "\n",
    "z_mean_test, z_log_var_test, z_test = vae.encoder.predict(X_test_final_scaled)\n",
    "\n",
    "reconstructions_test = vae.predict(X_test_final_scaled)\n",
    "\n",
    "print(f\"Shape of z_mean_test: {z_mean_test.shape}\")\n",
    "print(f\"Shape of z_log_var_test: {z_log_var_test.shape}\")\n",
    "print(f\"Shape of z_test: {z_test.shape}\")\n",
    "print(f\"Shape of reconstructions_test: {reconstructions_test.shape}\")\n",
    "print(\"Predictions completed.\")\n",
    "\n",
    "print(\"Calculating reconstruction errors for the test set...\")\n",
    "\n",
    "squared_diff = tf.square(X_test_final_scaled - reconstructions_test)\n",
    "\n",
    "mse_per_sample = tf.reduce_mean(squared_diff, axis=1)\n",
    "\n",
    "reconstruction_errors_test = mse_per_sample * original_dim\n",
    "\n",
    "print(f\"Shape of reconstruction_errors_test: {reconstruction_errors_test.shape}\")\n",
    "print(\"Reconstruction errors calculated successfully.\")\n",
    "\n",
    "print(\"Calculating KL divergence for the test set...\")\n",
    "\n",
    "kl_divergence_test = -0.5 * K.sum(1 + z_log_var_test - K.square(z_mean_test) - K.exp(z_log_var_test), axis=1)\n",
    "\n",
    "print(f\"Shape of kl_divergence_test: {kl_divergence_test.shape}\")\n",
    "print(\"KL divergence calculated successfully.\")\n",
    "\n",
    "print(\"Combining reconstruction error and KL divergence to calculate anomaly scores...\")\n",
    "\n",
    "reconstruction_errors_test_float32 = tf.cast(reconstruction_errors_test, tf.float32)\n",
    "kl_divergence_test_float32 = tf.cast(kl_divergence_test, tf.float32)\n",
    "\n",
    "anomaly_scores_test = reconstruction_errors_test_float32 + kl_divergence_test_float32\n",
    "\n",
    "print(f\"Shape of anomaly_scores_test: {anomaly_scores_test.shape}\")\n",
    "print(\"Anomaly scores calculated successfully.\")\n",
    "\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Evaluating anomaly detection performance using AUC-PR...\")\n",
    "auc_pr = average_precision_score(y_test_final, anomaly_scores_test)\n",
    "\n",
    "print(f\"Average Precision (AUC-PR): {auc_pr:.4f}\")\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test_final, anomaly_scores_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'AUC-PR = {auc_pr:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve for Anomaly Detection')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Anomaly detection performance evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
