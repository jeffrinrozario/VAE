{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ac3d3-24b8-49f8-8635-451ef520deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "latent_dim = 16\n",
    "intermediate_dim = 128\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "normal_class_label = 'normal.'\n",
    "\n",
    "original_data_dim = x_preprocessed.shape[1]\n",
    "\n",
    "x_train_normal_train, x_train_normal_val = train_test_split(\n",
    "    x_train_normal_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "x_train_normal_train = x_train_normal_train.values.astype(np.float32)\n",
    "x_train_normal_val = x_train_normal_val.values.astype(np.float32)\n",
    "\n",
    "x_test_processed = x_preprocessed.values.astype(np.float32)\n",
    "\n",
    "y_test = (y_labels != normal_class_label).astype(int)\n",
    "\n",
    "print(f\"Loaded x_train_normal_train with shape: {x_train_normal_train.shape}, dtype: {x_train_normal_train.dtype}\")\n",
    "print(f\"Loaded x_train_normal_val with shape: {x_train_normal_val.shape}, dtype: {x_train_normal_val.dtype}\")\n",
    "print(f\"Loaded x_test_processed with shape: {x_test_processed.shape}, dtype: {x_test_processed.dtype}\")\n",
    "print(f\"Created y_test with shape: {y_test.shape}, dtype: {y_test.dtype}, unique values: {np.unique(y_test)}\")\n",
    "\n",
    "print(f\"latent_dim: {latent_dim}\")\n",
    "print(f\"intermediate_dim: {intermediate_dim}\")\n",
    "print(f\"epochs: {epochs}\")\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "print(f\"normal_class_label: {normal_class_label}\")\n",
    "print(f\"original_data_dim: {original_data_dim}\")\n",
    "\n",
    "print(f\"Shape of x_train_normal_train: {x_train_normal_train.shape}, dtype: {x_train_normal_train.dtype}\")\n",
    "print(f\"Shape of x_train_normal_val: {x_train_normal_val.shape}, dtype: {x_train_normal_val.dtype}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Min value of x_train_normal_train: {np.min(x_train_normal_train)}\")\n",
    "print(f\"Max value of x_train_normal_train: {np.max(x_train_normal_train)}\")\n",
    "print(f\"Mean value of x_train_normal_train: {np.mean(x_train_normal_train)}\")\n",
    "\n",
    "print(f\"Shape of x_test_processed: {x_test_processed.shape}, dtype: {x_test_processed.dtype}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}, dtype: {y_test.dtype}\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print(\"TensorFlow, Keras layers, and Model imported successfully.\")\n",
    "\n",
    "original_dim = x_train_normal_train.shape[1]\n",
    "\n",
    "\n",
    "encoder_inputs = Input(shape=(original_dim,), name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(encoder_inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "encoder = Model(encoder_inputs, [z_mean, z_log_var], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "class Reparameterization(Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "print(\"Reparameterization trick layer defined.\")\n",
    "\n",
    "decoder_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(decoder_inputs)\n",
    "reconstruction = Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = Model(decoder_inputs, reconstruction, name='decoder')\n",
    "\n",
    "decoder.summary()\n",
    "\n",
    "vae_inputs = Input(shape=(original_dim,), name='vae_input')\n",
    "z_mean, z_log_var = encoder(vae_inputs)\n",
    "z = Reparameterization()([z_mean, z_log_var])\n",
    "reconstruction = decoder(z)\n",
    "\n",
    "vae = Model(vae_inputs, [reconstruction, z_mean, z_log_var], name='vae')\n",
    "\n",
    "vae.summary()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class KLDivergenceLossLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        kl_loss_per_sample = 0.5 * tf.reduce_sum(\n",
    "            tf.exp(z_log_var) + tf.square(z_mean) - 1.0 - z_log_var,\n",
    "            axis=1\n",
    "        )\n",
    "        self.add_loss(tf.reduce_mean(kl_loss_per_sample))\n",
    "        return z_mean\n",
    "\n",
    "vae_inputs = Input(shape=(original_dim,), name='vae_input')\n",
    "z_mean, z_log_var = encoder(vae_inputs)\n",
    "\n",
    "_ = KLDivergenceLossLayer(name='kl_divergence_calculator')([z_mean, z_log_var])\n",
    "\n",
    "z = Reparameterization()([z_mean, z_log_var])\n",
    "reconstruction = decoder(z)\n",
    "\n",
    "vae = Model(vae_inputs, reconstruction, name='vae')\n",
    "\n",
    "reconstruction_loss_fn = tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=False, reduction='sum'\n",
    ")\n",
    "\n",
    "vae.compile(optimizer='adam', loss=reconstruction_loss_fn)\n",
    "\n",
    "print(\"VAE model reassembled and compiled.\")\n",
    "\n",
    "history = vae.fit(\n",
    "    x=x_train_normal_train,\n",
    "    y=x_train_normal_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_train_normal_val, x_train_normal_val)\n",
    ")\n",
    "\n",
    "print(\"VAE model training complete.\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_reconstruction_error(original_input, reconstruction):\n",
    "    mse_loss = tf.keras.losses.MeanSquaredError(reduction='none')\n",
    "    reconstruction_error = tf.reduce_sum(mse_loss(original_input, reconstruction), axis=1)\n",
    "    return reconstruction_error\n",
    "\n",
    "print(\"Reconstruction error function defined (using MSE).\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_kl_divergence(input_data, encoder):\n",
    "    z_mean, z_log_var = encoder(input_data)\n",
    "\n",
    "    kl_loss_per_sample = 0.5 * tf.reduce_sum(\n",
    "        tf.exp(z_log_var) + tf.square(z_mean) - 1.0 - z_log_var,\n",
    "        axis=1\n",
    "    )\n",
    "    return kl_loss_per_sample\n",
    "\n",
    "print(\"KL divergence calculation function defined.\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_anomaly_score(input_data, vae_model, encoder):\n",
    "    reconstruction, z_mean, z_log_var = vae_model(input_data)\n",
    "\n",
    "    reconstruction_error = calculate_reconstruction_error(input_data, reconstruction)\n",
    "\n",
    "    kl_divergence = calculate_kl_divergence(input_data, encoder)\n",
    "\n",
    "    total_anomaly_score = reconstruction_error + kl_divergence\n",
    "    return total_anomaly_score\n",
    "\n",
    "print(\"Anomaly score calculation function defined.\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_anomaly_score(input_data, vae_model, encoder):\n",
    "    reconstruction = vae_model(input_data)\n",
    "    reconstruction_error = calculate_reconstruction_error(input_data, reconstruction)\n",
    "    kl_divergence = calculate_kl_divergence(input_data, encoder)\n",
    "\n",
    "    total_anomaly_score = reconstruction_error + kl_divergence\n",
    "    return total_anomaly_score\n",
    "\n",
    "print(\"Anomaly score calculation function defined (fixed for VAE output).\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_reconstruction_error(original_input, reconstruction):\n",
    "    squared_difference = tf.square(original_input - reconstruction)\n",
    "    reconstruction_error = tf.reduce_sum(squared_difference, axis=1)\n",
    "    return reconstruction_error\n",
    "\n",
    "print(\"Reconstruction error function defined (using explicit squared difference sum).\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_test_processed_tf = tf.constant(x_test_processed, dtype=tf.float32)\n",
    "anomaly_scores = calculate_anomaly_score(x_test_processed_tf, vae, encoder)\n",
    "\n",
    "normal_mask = (y_test == normal_class_label)\n",
    "anomalous_mask = (y_test != normal_class_label)\n",
    "\n",
    "anomaly_scores_normal = anomaly_scores[normal_mask]\n",
    "anomaly_scores_anomalous = anomaly_scores[anomalous_mask]\n",
    "\n",
    "print(f\"Calculated {len(anomaly_scores)} anomaly scores for the test set.\")\n",
    "print(f\"Number of normal samples: {len(anomaly_scores_normal)}\")\n",
    "print(f\"Number of anomalous samples: {len(anomaly_scores_anomalous)}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(anomaly_scores_normal.numpy(), color='blue', label='Normal Samples', kde=True, stat='density', alpha=0.5, bins=50)\n",
    "sns.histplot(anomaly_scores_anomalous.numpy(), color='red', label='Anomalous Samples', kde=True, stat='density', alpha=0.5, bins=50)\n",
    "\n",
    "plt.title('Distribution of Anomaly Scores for Normal vs. Anomalous Samples')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Anomaly score distributions visualized.\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "anomaly_threshold = np.percentile(anomaly_scores_normal, 95)\n",
    "\n",
    "print(f\"Calculated anomaly detection threshold (95th percentile of normal scores): {anomaly_threshold:.2f}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_pred = (anomaly_scores > anomaly_threshold).numpy().astype(int)\n",
    "anomalous_count = np.sum(y_pred)\n",
    "\n",
    "print(f\"Anomaly detection threshold: {anomaly_threshold:.2f}\")\n",
    "print(f\"Number of samples classified as anomalous by this threshold: {anomalous_count}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_true_binary = (y_test != normal_class_label).astype(int)\n",
    "\n",
    "print(f\"Original y_test unique values: {np.unique(y_test)}\")\n",
    "print(f\"Binary y_true_binary unique values: {np.unique(y_true_binary)}\")\n",
    "print(f\"Number of true normal samples: {np.sum(y_true_binary == 0)}\")\n",
    "print(f\"Number of true anomalous samples: {np.sum(y_true_binary == 1)}\")\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "print(\"Scikit-learn metrics imported successfully.\")\n",
    "\n",
    "precision = precision_score(y_true_binary, y_pred)\n",
    "recall = recall_score(y_true_binary, y_pred)\n",
    "f1 = f1_score(y_true_binary, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "auc_pr = average_precision_score(y_true_binary, anomaly_scores)\n",
    "\n",
    "print(f\"Area Under the Precision-Recall Curve (AUC-PR): {auc_pr:.4f}\")\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_true_binary, anomaly_scores)\n",
    "\n",
    "print(\"Precision-Recall curve points computed.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'AUC-PR: {auc_pr:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Precision-Recall curve plotted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
