{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a7d3e43-3aa1-4346-a7ce-d804f87a043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy x_train_normal_train with shape: (4800, 784)\n",
      "Created dummy x_train_normal_val with shape: (1200, 784)\n",
      "Created dummy x_test_processed with shape: (10000, 784)\n",
      "Created dummy y_test with shape: (10000,) and unique values: [0 1]\n",
      "latent_dim: 16\n",
      "intermediate_dim: 128\n",
      "epochs: 10\n",
      "batch_size: 32\n",
      "normal_class_label: 0\n",
      "Shape of x_train_normal_train: (4800, 784), dtype: float32\n",
      "Shape of x_train_normal_val: (1200, 784), dtype: float32\n",
      "Min value of x_train_normal_train: 2.6456815405140333e-08\n",
      "Max value of x_train_normal_train: 0.9999999403953552\n",
      "Mean value of x_train_normal_train: 0.49982786178588867\n",
      "Shape of x_test_processed: (10000, 784), dtype: float32\n",
      "Shape of y_test: (10000,), dtype: int64\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape of x_test_processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_test_processed.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_test_processed.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape of y_test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Layer, Dense, Input\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "latent_dim = 16\n",
    "intermediate_dim = 128\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "normal_class_label = 0\n",
    "original_data_dim = 784\n",
    "\n",
    "x_train_normal_train = np.random.rand(4800, original_data_dim).astype(np.float32)\n",
    "print(f\"Created dummy x_train_normal_train with shape: {x_train_normal_train.shape}\")\n",
    "\n",
    "x_train_normal_val = np.random.rand(1200, original_data_dim).astype(np.float32)\n",
    "print(f\"Created dummy x_train_normal_val with shape: {x_train_normal_val.shape}\")\n",
    "\n",
    "x_test_processed = np.random.rand(10000, original_data_dim).astype(np.float32)\n",
    "print(f\"Created dummy x_test_processed with shape: {x_test_processed.shape}\")\n",
    "\n",
    "y_test = np.full(10000, normal_class_label, dtype=int)\n",
    "num_anomalies = int(0.1 * 10000)\n",
    "anomaly_indices = np.random.choice(10000, num_anomalies, replace=False)\n",
    "y_test[anomaly_indices] = 1\n",
    "print(f\"Created dummy y_test with shape: {y_test.shape} and unique values: {np.unique(y_test)}\")\n",
    "\n",
    "print(f\"latent_dim: {latent_dim}\")\n",
    "print(f\"intermediate_dim: {intermediate_dim}\")\n",
    "print(f\"epochs: {epochs}\")\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "print(f\"normal_class_label: {normal_class_label}\")\n",
    "\n",
    "print(f\"Shape of x_train_normal_train: {x_train_normal_train.shape}, dtype: {x_train_normal_train.dtype}\")\n",
    "print(f\"Shape of x_train_normal_val: {x_train_normal_val.shape}, dtype: {x_train_normal_val.dtype}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Min value of x_train_normal_train: {np.min(x_train_normal_train)}\")\n",
    "print(f\"Max value of x_train_normal_train: {np.max(x_train_normal_train)}\")\n",
    "print(f\"Mean value of x_train_normal_train: {np.mean(x_train_normal_train)}\")\n",
    "\n",
    "print(f\"Shape of x_test_processed: {x_test_processed.shape}, dtype: {x_test_processed.dtype}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}, dtype: {y_test.dtype}\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print(\"TensorFlow, Keras layers, and Model imported successfully.\")\n",
    "\n",
    "original_dim = x_train_normal_train.shape[1]\n",
    "\n",
    "\n",
    "encoder_inputs = Input(shape=(original_dim,), name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(encoder_inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "encoder = Model(encoder_inputs, [z_mean, z_log_var], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "class Reparameterization(Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "print(\"Reparameterization trick layer defined.\")\n",
    "\n",
    "decoder_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(decoder_inputs)\n",
    "reconstruction = Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = Model(decoder_inputs, reconstruction, name='decoder')\n",
    "\n",
    "decoder.summary()\n",
    "\n",
    "vae_inputs = Input(shape=(original_dim,), name='vae_input')\n",
    "z_mean, z_log_var = encoder(vae_inputs)\n",
    "z = Reparameterization()([z_mean, z_log_var])\n",
    "reconstruction = decoder(z)\n",
    "\n",
    "vae = Model(vae_inputs, [reconstruction, z_mean, z_log_var], name='vae')\n",
    "\n",
    "vae.summary()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class KLDivergenceLossLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        kl_loss_per_sample = 0.5 * tf.reduce_sum(\n",
    "            tf.exp(z_log_var) + tf.square(z_mean) - 1.0 - z_log_var,\n",
    "            axis=1\n",
    "        )\n",
    "        self.add_loss(tf.reduce_mean(kl_loss_per_sample))\n",
    "        return z_mean\n",
    "\n",
    "vae_inputs = Input(shape=(original_dim,), name='vae_input')\n",
    "z_mean, z_log_var = encoder(vae_inputs)\n",
    "\n",
    "_ = KLDivergenceLossLayer(name='kl_divergence_calculator')([z_mean, z_log_var])\n",
    "\n",
    "z = Reparameterization()([z_mean, z_log_var])\n",
    "reconstruction = decoder(z)\n",
    "\n",
    "vae = Model(vae_inputs, reconstruction, name='vae')\n",
    "\n",
    "reconstruction_loss_fn = tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=False, reduction='sum'\n",
    ")\n",
    "\n",
    "vae.compile(optimizer='adam', loss=reconstruction_loss_fn)\n",
    "\n",
    "print(\"VAE model reassembled and compiled.\")\n",
    "\n",
    "history = vae.fit(\n",
    "    x=x_train_normal_train,\n",
    "    y=x_train_normal_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_train_normal_val, x_train_normal_val)\n",
    ")\n",
    "\n",
    "print(\"VAE model training complete.\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_reconstruction_error(original_input, reconstruction):\n",
    "    mse_loss = tf.keras.losses.MeanSquaredError(reduction='none')\n",
    "    reconstruction_error = tf.reduce_sum(mse_loss(original_input, reconstruction), axis=1)\n",
    "    return reconstruction_error\n",
    "\n",
    "print(\"Reconstruction error function defined (using MSE).\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_kl_divergence(input_data, encoder):\n",
    "    z_mean, z_log_var = encoder(input_data)\n",
    "\n",
    "    kl_loss_per_sample = 0.5 * tf.reduce_sum(\n",
    "        tf.exp(z_log_var) + tf.square(z_mean) - 1.0 - z_log_var,\n",
    "        axis=1\n",
    "    )\n",
    "    return kl_loss_per_sample\n",
    "\n",
    "print(\"KL divergence calculation function defined.\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_anomaly_score(input_data, vae_model, encoder):\n",
    "    reconstruction, z_mean, z_log_var = vae_model(input_data)\n",
    "\n",
    "    reconstruction_error = calculate_reconstruction_error(input_data, reconstruction)\n",
    "\n",
    "    kl_divergence = calculate_kl_divergence(input_data, encoder)\n",
    "\n",
    "    total_anomaly_score = reconstruction_error + kl_divergence\n",
    "    return total_anomaly_score\n",
    "\n",
    "print(\"Anomaly score calculation function defined.\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_anomaly_score(input_data, vae_model, encoder):\n",
    "    reconstruction = vae_model(input_data)\n",
    "    reconstruction_error = calculate_reconstruction_error(input_data, reconstruction)\n",
    "    kl_divergence = calculate_kl_divergence(input_data, encoder)\n",
    "\n",
    "    total_anomaly_score = reconstruction_error + kl_divergence\n",
    "    return total_anomaly_score\n",
    "\n",
    "print(\"Anomaly score calculation function defined (fixed for VAE output).\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def calculate_reconstruction_error(original_input, reconstruction):\n",
    "    squared_difference = tf.square(original_input - reconstruction)\n",
    "    reconstruction_error = tf.reduce_sum(squared_difference, axis=1)\n",
    "    return reconstruction_error\n",
    "\n",
    "print(\"Reconstruction error function defined (using explicit squared difference sum).\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_test_processed_tf = tf.constant(x_test_processed, dtype=tf.float32)\n",
    "anomaly_scores = calculate_anomaly_score(x_test_processed_tf, vae, encoder)\n",
    "\n",
    "normal_mask = (y_test == normal_class_label)\n",
    "anomalous_mask = (y_test != normal_class_label)\n",
    "\n",
    "anomaly_scores_normal = anomaly_scores[normal_mask]\n",
    "anomaly_scores_anomalous = anomaly_scores[anomalous_mask]\n",
    "\n",
    "print(f\"Calculated {len(anomaly_scores)} anomaly scores for the test set.\")\n",
    "print(f\"Number of normal samples: {len(anomaly_scores_normal)}\")\n",
    "print(f\"Number of anomalous samples: {len(anomaly_scores_anomalous)}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(anomaly_scores_normal.numpy(), color='blue', label='Normal Samples', kde=True, stat='density', alpha=0.5, bins=50)\n",
    "sns.histplot(anomaly_scores_anomalous.numpy(), color='red', label='Anomalous Samples', kde=True, stat='density', alpha=0.5, bins=50)\n",
    "\n",
    "plt.title('Distribution of Anomaly Scores for Normal vs. Anomalous Samples')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Anomaly score distributions visualized.\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "anomaly_threshold = np.percentile(anomaly_scores_normal, 95)\n",
    "\n",
    "print(f\"Calculated anomaly detection threshold (95th percentile of normal scores): {anomaly_threshold:.2f}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_pred = (anomaly_scores > anomaly_threshold).numpy().astype(int)\n",
    "anomalous_count = np.sum(y_pred)\n",
    "\n",
    "print(f\"Anomaly detection threshold: {anomaly_threshold:.2f}\")\n",
    "print(f\"Number of samples classified as anomalous by this threshold: {anomalous_count}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_true_binary = (y_test != normal_class_label).astype(int)\n",
    "\n",
    "print(f\"Original y_test unique values: {np.unique(y_test)}\")\n",
    "print(f\"Binary y_true_binary unique values: {np.unique(y_true_binary)}\")\n",
    "print(f\"Number of true normal samples: {np.sum(y_true_binary == 0)}\")\n",
    "print(f\"Number of true anomalous samples: {np.sum(y_true_binary == 1)}\")\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "print(\"Scikit-learn metrics imported successfully.\")\n",
    "\n",
    "precision = precision_score(y_true_binary, y_pred)\n",
    "recall = recall_score(y_true_binary, y_pred)\n",
    "f1 = f1_score(y_true_binary, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "auc_pr = average_precision_score(y_true_binary, anomaly_scores)\n",
    "\n",
    "print(f\"Area Under the Precision-Recall Curve (AUC-PR): {auc_pr:.4f}\")\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_true_binary, anomaly_scores)\n",
    "\n",
    "print(\"Precision-Recall curve points computed.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'AUC-PR: {auc_pr:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Precision-Recall curve plotted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c4eea-687c-4318-b10d-462ed67631df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
